# Policy Optimization with Constraint Advantage Regularization (POCAR)

# Current status
All enviroment share the same GPPO, RPPO, APPO and our-PPO code base. 
Downside: difficult to extract enviroment specific information, such as "incident rate"

# Original code
Authors: Eric Yang Yu, Zhizhen Qin, Min Kyung Lee, Sicun Gao \
NeurIPS (Conference on Neural Information Processing Systems) 2022

This is the code implementation for the NeurIPS 2022 [paper](https://arxiv.org/abs/2210.12546) above. 
Code and environments are adapted from the original Google ML-fairness-gym [repo](https://github.com/google/ml-fairness-gym).
